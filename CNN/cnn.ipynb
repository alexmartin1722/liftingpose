{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source\n",
    "-- https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/04_pytorch_custom_datasets_exercise_solutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from cnnDataset import CNNDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizer = 1\n",
    "transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Resize([64*resizer, 64*resizer]), # This line\n",
    "                               torchvision.transforms.Normalize((.5,.5,.5), (.5,.5,.5))\n",
    "                                ])\n",
    "# load dataset \n",
    "# csv_path = 'C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\cnn_name_class.csv'\n",
    "# data_path = 'C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\Data_Cnn'\n",
    "\n",
    "# dataset = CNNDataset(csv_file='cnn_name_class.csv', root_dir='Data_cnn', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:\\\\Users\\\\amart50\\\\Desktop\\\\ViTPose\\\\298_scripts\\\\CNN_test\\\\data'\n",
    "dataset = ImageFolder(data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840 210 1050 1050\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "train_split = int( len(dataset) * .8)\n",
    "test_split = len(dataset)- train_split\n",
    "print(train_split, test_split, train_split+test_split, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_split, test_split])\n",
    "trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Bench', 'Deadlift', 'Squat'],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1f94f13e890>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1f94ed45690>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.__getitem__(0)\n",
    "dataset.classes, trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape:torch.Size([1, 3, 64, 64]) (B, corlorchannel, H, W)\n",
      "Image Shape:torch.Size([1]) (B, corlorchannel, H, W)\n"
     ]
    }
   ],
   "source": [
    "img, label = next(iter(trainloader))\n",
    "print(f\"Image Shape:{img.shape} (B, corlorchannel, H, W)\")\n",
    "print(f\"Image Shape:{label.shape} (B, corlorchannel, H, W)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grid_img = torchvision.utils.make_grid(images[:4])\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(grid_img.permute(1, 2, 0))\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "show_batch(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "  def __init__(self, input_shape, hidden_units, output_shape):\n",
    "    super().__init__()\n",
    "    self.conv_block_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_shape,\n",
    "                  out_channels=2* hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=2* hidden_units,\n",
    "                  out_channels=4* hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    self.conv_block_2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=4* hidden_units,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=hidden_units,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=hidden_units*16*16,\n",
    "                  out_features=output_shape))\n",
    "        \n",
    "  def forward(self, x):\n",
    "    x = self.conv_block_1(x)\n",
    "    # print(f\"Layer 1 shape: {x.shape}\")\n",
    "    x = self.conv_block_2(x)\n",
    "    # print(f\"Layer 2 shape: {x.shape}\")\n",
    "    x = self.classifier(x)\n",
    "    # print(f\"Layer 3 shape: {x.shape}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=4096, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0 = TinyVGG(input_shape = 3,\n",
    "                  hidden_units=16,\n",
    "                  output_shape=len(dataset.classes)).to(device)\n",
    "model_0\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0395, -0.0189, -0.0654]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass dummy data through model\n",
    "dummy_x = torch.rand(size=[1, 3, 64, 64])\n",
    "model_0(dummy_x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  \n",
    "  # Put the model in train mode\n",
    "  model.train()\n",
    "\n",
    "  # Setup train loss and train accuracy values\n",
    "  train_loss, train_acc = 0, 0\n",
    "\n",
    "  # Loop through data loader and data batches\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Send data to target device\n",
    "    X, y = X.to(device), y.to(device) \n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_pred = model(X)\n",
    "    # print(y_pred)\n",
    "\n",
    "    # 2. Calculate and accumulate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    # 3. Optimizer zero grad \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward \n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate and accumualte accuracy metric across all batches\n",
    "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "  # Adjust metrics to get average loss and average accuracy per batch\n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_acc = train_acc / len(dataloader)\n",
    "  return train_loss, train_acc \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "  \n",
    "  # Put model in eval mode\n",
    "  model.eval()\n",
    "\n",
    "  # Setup the test loss and test accuracy values\n",
    "  test_loss, test_acc = 0, 0\n",
    "\n",
    "  # Turn on inference context manager\n",
    "  with torch.inference_mode():\n",
    "    # Loop through DataLoader batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "      # Send data to target device\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # 1. Forward pass\n",
    "      test_pred_logits = model(X)\n",
    "      # print(test_pred_logits)\n",
    "\n",
    "      # 2. Calculuate and accumulate loss\n",
    "      loss = loss_fn(test_pred_logits, y)\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      # Calculate and accumulate accuracy\n",
    "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "    \n",
    "  # Adjust metrics to get average loss and accuracy per batch\n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  test_acc = test_acc / len(dataloader)\n",
    "  return test_loss, test_acc\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "  \n",
    "  # Create results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "             \"train_acc\": [],\n",
    "             \"test_loss\": [],\n",
    "             \"test_acc\": []}\n",
    "\n",
    "  # Loop through the training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    # Train step\n",
    "    train_loss, train_acc = train_step(model=model, \n",
    "                                       dataloader=train_dataloader,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       optimizer=optimizer)\n",
    "    # Test step\n",
    "    test_loss, test_acc = test_step(model=model, \n",
    "                                    dataloader=test_dataloader,\n",
    "                                    loss_fn=loss_fn)\n",
    "    \n",
    "    # Print out what's happening\n",
    "    print(f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Update the results dictionary\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "    results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "  # Return the results dictionary\n",
    "  return results\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = TinyVGG(input_shape=3,\n",
    "                  hidden_units=10,\n",
    "                  output_shape=len(dataset.classes)).to(device)\n",
    "\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0853 | train_acc: 0.4250 | test_loss: 1.0897 | test_acc: 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_0_results = train(model=model_0,\n",
    "                        train_dataloader=trainloader,\n",
    "                        test_dataloader=testloader,\n",
    "                        optimizer=optimizer,\n",
    "                        epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'to_device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mamart50\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDesktop\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mliftingpose\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCNN\u001b[39;00m \u001b[39mimport\u001b[39;00m ResNet, block\n\u001b[1;32m----> 5\u001b[0m model_1 \u001b[39m=\u001b[39m ResNet(in_channels\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, block\u001b[39m=\u001b[39;49mblock, layers\u001b[39m=\u001b[39;49m[\u001b[39m3\u001b[39;49m,\u001b[39m4\u001b[39;49m,\u001b[39m6\u001b[39;49m,\u001b[39m3\u001b[39;49m], image_channels\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, num_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39;49mto_device()\n\u001b[0;32m      7\u001b[0m \u001b[39m#train model 1\u001b[39;00m\n\u001b[0;32m      8\u001b[0m train(model\u001b[39m=\u001b[39mmodel_1,\n\u001b[0;32m      9\u001b[0m         train_dataloader\u001b[39m=\u001b[39mtrainloader,\n\u001b[0;32m     10\u001b[0m         test_dataloader\u001b[39m=\u001b[39mtestloader,\n\u001b[0;32m     11\u001b[0m         optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m     12\u001b[0m         epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amart50\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\torch\\nn\\modules\\module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1264\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1265\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1266\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'to_device'"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('C:\\\\Users\\\\amart50\\\\Desktop\\\\liftingpose')\n",
    "from CNN import ResNet, block\n",
    "\n",
    "model_1 = ResNet(in_channels=128, block=block, layers=[3,4,6,3], image_channels=3, num_classes=3)\n",
    "\n",
    "#train model 1\n",
    "train(model=model_1,\n",
    "        train_dataloader=trainloader,\n",
    "        test_dataloader=testloader,\n",
    "        optimizer=optimizer,\n",
    "        epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     losses = []\n",
    "#     for batch_idx, (data, targets) in enumerate(trainloader):\n",
    "#         # Get data to cuda if possible\n",
    "#         data = data.to(device=device)\n",
    "#         targets = targets.to(device=device)\n",
    "        \n",
    "#         # forward\n",
    "#         scores = model(data)\n",
    "#         loss = criterion(scores, targets)\n",
    "#         losses.append(loss.item())\n",
    "        \n",
    "#         # backward\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # gradient descent or adam step\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     print(f'Cost at epoch {epoch} is {sum(losses)/len(losses):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     \"\"\"\n",
    "#     CNN model for lift classification with 3 output layers classifying \n",
    "#     the lift as Squat, Bench, or Deadlift\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         #define a 3 layer CNN\n",
    "#         self.conv1 = nn.Conv2d(3,1 , 256) # 1 input channel, 32 output channels, 3x3 kernel, stride 1, padding 1\n",
    "#         self.conv2 = nn.Conv2d(256, 256*2, 5) # 32 input channels, 64 output channels, 3x3 kernel, stride 1, padding 1\n",
    "#         self.maxpool_layer = nn.MaxPool2d(3, 3) # 2x2 kernel, stride 2\n",
    "#         self.fc_layer = nn.Linear(256*5*5, 3) # 64*5*5 input features, 3 output features (for 3 classes)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.maxpool_layer(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.maxpool_layer(x)\n",
    "#         x = x.view(-1, 256*5*5) \n",
    "#         x = torch.sigmoid(self.fc_layer(x))\n",
    "#         return x\n",
    "    \n",
    "\n",
    "#     # def test_one(self, image, label):\n",
    "#     #     outputs = self(image)\n",
    "#     #     _, predicted = torch.max(outputs.data, 1)\n",
    "#     #     print('Predicted: ', predicted.item(), 'Actual: ', label.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'Classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# import cv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# x = iter(trainloader)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# images, labels = next(x)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m trainloader\u001b[39m.\u001b[39;49mClasses\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'Classes'"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# x = iter(trainloader)\n",
    "# images, labels = next(x)\n",
    "trainloader.Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grid_img = torchvision.utils.make_grid(images[:4])\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(grid_img.permute(1, 2, 0))\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "show_batch(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the dimensions of the input image\n",
    "width = 256\n",
    "height = 256\n",
    "\n",
    "# Define the number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "    self.fc1 = torch.nn.Linear(64 * width * height, 512)\n",
    "    self.fc2 = torch.nn.Linear(512, num_classes)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = torch.nn.functional.relu(self.conv1(x))\n",
    "    x = torch.nn.functional.relu(self.conv2(x))\n",
    "    x = torch.nn.functional.relu(self.conv3(x))\n",
    "    x = x.view(-1, 64 * width * height)\n",
    "    x = torch.nn.functional.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "  # Set the model to training mode\n",
    "  model.train()\n",
    "\n",
    "  # Initialize the average loss\n",
    "  avg_loss = 0.0\n",
    "\n",
    "  # Loop over the training data\n",
    "  for images, labels in data:\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize the model's parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the average loss\n",
    "    avg_loss += loss.item()\n",
    "\n",
    "  # Return the average loss\n",
    "  return avg_loss / len(data)\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # Initialize the average accuracy\n",
    "  avg_accuracy = 0.0\n",
    "\n",
    "  # Loop over the validation data\n",
    "  for images, labels in data:\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    _, predictions = torch.max(outputs, dim=1)\n",
    "    accuracy = torch.mean(predictions.eq(labels).float())\n",
    "\n",
    "    # Update the average accuracy\n",
    "    avg_accuracy += accuracy.item()\n",
    "\n",
    "  # Return the average accuracy\n",
    "  return avg_accuracy / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amart50\\Desktop\\CNN_test\\cnn.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m valid_accuracies \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   \u001b[39m# Train the model for one epoch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   train_accuracy \u001b[39m=\u001b[39m train(model, trainloader, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   train_accuracies\u001b[39m.\u001b[39mappend(train_accuracy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   \u001b[39m# Evaluate the model on the validation set\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\amart50\\Desktop\\CNN_test\\cnn.ipynb Cell 19\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data, optimizer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Optimize the model's parameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Update the average loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model and record its accuracy on the training set and validation set\n",
    "# at each epoch\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "  # Train the model for one epoch\n",
    "  train_accuracy = train(model, trainloader, optimizer)\n",
    "  train_accuracies.append(train_accuracy)\n",
    "  \n",
    "  # Evaluate the model on the validation set\n",
    "  valid_accuracy = evaluate(model, testloader)\n",
    "  valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "# Create a plot of the model's accuracy versus the number of epochs\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label=\"Train\")\n",
    "plt.plot(range(1, num_epochs + 1), valid_accuracies, label=\"Valid\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the initial learning rate here\n",
    "# learning_rate = 1e-2\n",
    "# n_epochs = 30 # how many epochs to run\n",
    "\n",
    "# # define loss function\n",
    "# criterion = nn.BCELoss()\n",
    "# cnn_net = Net()\n",
    "# optimizer = torch.optim.SGD(cnn_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 1):\n",
    "\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data\n",
    "#         labels = labels.float()\n",
    "\n",
    "#         # Forward \n",
    "#         output = cnn_net(inputs)\n",
    "        \n",
    "#         # Compute the loss using the final output\n",
    "#         loss = criterion(output, labels)\n",
    "\n",
    "#         # Backpropagation\n",
    "#         # YOUR CODE HERE\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         #raise NotImplementedError()\n",
    "        \n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 200 == 199:  # print every 200 mini-batches\n",
    "#             print('[Epoch %d, Step %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 200))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CnNDataset debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "annotations = pd.read_csv('cnn_name_class.csv')\n",
    "root_dir = 'C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\Data_Cnn'\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\Data_Cnn', 'bench1.jpg')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir, annotations.iloc[index, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\Data_Cnn\\\\bench1.jpg'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = os.path.join(root_dir, annotations.iloc[index, 1])\n",
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = annotations['Names']\n",
    "new_name = []\n",
    "for names in name:\n",
    "    names = names.split(\"/\")[1]\n",
    "    new_name.append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bench/bench0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bench/bench1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bench/bench10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bench/bench100.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bench/bench101.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Names  Class\n",
       "0    bench/bench0.jpg      0\n",
       "1    bench/bench1.jpg      0\n",
       "2   bench/bench10.jpg      0\n",
       "3  bench/bench100.jpg      0\n",
       "4  bench/bench101.jpg      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['Names'].replace(new_name)\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1043, 1043)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations), len(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bench/bench0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bench/bench1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bench/bench10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bench/bench100.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bench/bench101.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Names  Class\n",
       "0    bench/bench0.jpg      0\n",
       "1    bench/bench1.jpg      0\n",
       "2   bench/bench10.jpg      0\n",
       "3  bench/bench100.jpg      0\n",
       "4  bench/bench101.jpg      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[\"Names\"] = new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.to_csv(\"cnn_name_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('cnn_notebook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09b4fa13e1113080a851f86ceca2c5ccb735b8c635027d206146bbec8c01cbde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
